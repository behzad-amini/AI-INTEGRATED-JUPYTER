{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/behzad-amini/AI-INTEGRATED-JUPYTER/blob/main/Publish_LLM_With_Ngrok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we are gonna load our model and make an OpenAI compatible API for it using the Flask. Then we will publish the API using Ngrok free service."
      ],
      "metadata": {
        "id": "bxxUmIZhF55m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLqSX-Qn2m7i"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers accelerate optimum auto-gptq\n",
        "!pip install flask pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Your Model"
      ],
      "metadata": {
        "id": "jNdLCIDE6orJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")"
      ],
      "metadata": {
        "id": "DrPQgBqc6loe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Response"
      ],
      "metadata": {
        "id": "IsBD7PC5-8GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_response(messages):\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True\n",
        "  )\n",
        "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  generated_ids = model.generate(\n",
        "      model_inputs.input_ids,\n",
        "      attention_mask=model_inputs.attention_mask,\n",
        "      max_new_tokens=512\n",
        "  )\n",
        "  generated_ids = [\n",
        "      output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "  ]\n",
        "\n",
        "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  return response"
      ],
      "metadata": {
        "id": "qAtfbWvW_E_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Publish The Model"
      ],
      "metadata": {
        "id": "JWRIhnHp_XvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Ngrok"
      ],
      "metadata": {
        "id": "wiTnuxS8_dnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should obtain your Ngrok auth token [here](https://dashboard.ngrok.com/tunnels/authtokens). And then set it in the environment variable `NGROK_TOKEN`."
      ],
      "metadata": {
        "id": "Y_JL3W65GPIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "port = 8000\n",
        "\n",
        "conf.get_default().auth_token = os.environ[\"NGROK_TOKEN\"]\n",
        "public_url = ngrok.connect(port).public_url\n",
        "\n",
        "print(f\"Ngrok Tunnel '{public_url}' -> 'http://127.0.0.1:{port}'\")"
      ],
      "metadata": {
        "id": "T4P_VLxG_bvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup API"
      ],
      "metadata": {
        "id": "_3r9vCqBAjXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "  return \"hello\"\n",
        "\n",
        "@app.route(\"/chat/completions\", methods=[\"POST\"])\n",
        "def chat_completions():\n",
        "    # Note: Better to validate the request data\n",
        "    data = request.json\n",
        "    resp_content = gen_response(data[\"messages\"])\n",
        "\n",
        "    response = {\n",
        "        \"id\": str(random.randint(111111, 999999)),\n",
        "        \"object\": \"chat.completion\",\n",
        "        \"created\": time.time(),\n",
        "        \"model\": data[\"model\"],\n",
        "        \"choices\": [{\n",
        "            \"message\": {\"role\": \"assistant\", \"content\": resp_content}\n",
        "        }]\n",
        "    }\n",
        "    return jsonify(response)\n",
        "\n",
        "app.run(port=port)"
      ],
      "metadata": {
        "id": "03CR4x8qBLFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test With OpenAI Client"
      ],
      "metadata": {
        "id": "hq9_qjA-Fa6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"<PUBLIC-URL>\",  # Copy the printed public url.\n",
        "    api_key=\"fake\",  # It will be ignored.\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is Ngrok\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"qwen2\",\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "6iYaSE_5FeHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}